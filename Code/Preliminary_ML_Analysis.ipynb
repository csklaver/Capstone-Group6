{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTiWqATn4ANO"
   },
   "source": [
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Predicting Depression - Preliminary Models (Imbalanced Data)\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jWmYBTOwgNs-",
    "outputId": "3886a80c-7341-4485-f2c4-9cfd4b1bb046"
   },
   "outputs": [],
   "source": [
    "path = ('/Users/carolinesklaver/Desktop/Capstone/NHANES/data/csv_data/')\n",
    "\n",
    "import os\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MUl4k83e4ANR"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBRVH9SB4ANb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eOpQpPu4ANk"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "df_raw = pd.read_csv('df_raw_v2.csv')\n",
    "\n",
    "# bring year and target col to the beginning of df\n",
    "year = df_raw.pop('year')\n",
    "df_raw.insert(1, 'year', year)\n",
    "\n",
    "dep = df_raw.pop('depressed')\n",
    "df_raw.insert(2, 'depressed', dep)\n",
    "\n",
    "\n",
    "\n",
    "# drop marijuana use\n",
    "df_raw.drop(['used_marijuana'],axis=1, inplace=True)\n",
    "# help!\n",
    "df_raw.drop(['year'],axis=1, inplace=True)\n",
    "\n",
    "df_raw.drop(['SEQN'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continuous features\n",
    "cont = ['#_ppl_household', 'age', 'triglyceride','caffeine', 'lifetime_partners',\n",
    "       'glycohemoglobin', 'CRP', 'tot_cholesterol','systolic_BP','diastolic_BP', 'BMI', 'waist_C', '#meals_fast_food',\n",
    "       'min_sedetary', 'bone_mineral_density']\n",
    "\n",
    "# categorical features\n",
    "cat = ['race_ethnicity', 'edu_level', 'gender', 'marital_status', 'annual_HI',\n",
    "       'doc_diabetes', 'how_healthy_diet', 'used_CMH',\n",
    "       'health_insurance', 'doc_asthma', 'doc_overweight', 'doc_arthritis',\n",
    "       'doc_CHF', 'doc_CHD', 'doc_heart_attack', 'doc_stroke',\n",
    "       'doc_chronic_bronchitis', 'doc_liver_condition', 'doc_thyroid_problem',\n",
    "       'doc_cancer', 'difficult_seeing', 'doc_kidney', 'broken_hip',\n",
    "       'doc_osteoporosis', 'vigorous_activity', 'moderate_activity',\n",
    "       'doc_sleeping_disorder', 'smoker', 'sexual_orientation',\n",
    "       'alcoholic','herpes_2', 'HIV', 'doc_HPV','difficult_hearing', 'doc_COPD']\n",
    "\n",
    "# target binary feature\n",
    "target = 'depressed'\n",
    "\n",
    "# multi-class features\n",
    "cat_encode = ['race_ethnicity', 'edu_level', 'gender', 'marital_status', 'annual_HI','how_healthy_diet',\n",
    "              'sexual_orientation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6onKpDeL4ANn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def nan_helper(df):\n",
    "    \"\"\"\n",
    "    The NaN helper\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of variables with NaN (index), \n",
    "    raw number missing, and their proportion\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # get the raw number of missing values & sort\n",
    "    missing = df.isnull().sum().sort_values(ascending=True)\n",
    "    \n",
    "    # get the proportion of missing values (%)\n",
    "    proportion = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=True)\n",
    "    \n",
    "    # create table of missing data\n",
    "    nan_data = pd.concat([missing, proportion], axis=1, keys=['missing', 'proportion'])\n",
    "    \n",
    "    return nan_data\n",
    "\n",
    "\n",
    "def missing_values(df, threshold_col, threshold_row, impute_type):\n",
    "    \"\"\"\n",
    "    Handle Missing Values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    threshold_col: the proportion of missing values at which  to drop whole column\n",
    "    threshold_row: the proportion of missing values at which to drop rows\n",
    "    impute_type: mean or median imputation for continuous variables\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe without missing values\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Dropping Cols and Rows\n",
    "    # call NaN helper function\n",
    "    df_nan = nan_helper(df)\n",
    "        \n",
    "    # drop columns with higher proportion missing than threshold col\n",
    "    df = df.drop((df_nan[df_nan['proportion'] > threshold_col]).index,1)\n",
    "    \n",
    "    # drop rows with higher proportion missing than threshold row\n",
    "    df_nan_2 = df_nan[df_nan['proportion']>threshold_row]\n",
    "    df = df.dropna(subset=np.intersect1d(df_nan_2.index, df.columns),\n",
    "                           inplace=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Imputing values\n",
    "    # Impute continuous variables with mean \n",
    "    if impute_type == 'mean':\n",
    "        for col in cont:\n",
    "            if col in df.columns:\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "    # Impute continuous variables with median\n",
    "    elif impute_type == 'median':\n",
    "        for col in cont:\n",
    "            if col in df.columns:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    \n",
    "    # Impute categorical variables with most frequent/mode\n",
    "    for col in cat:\n",
    "        if col in df.columns:\n",
    "            df[col].fillna(df[col].value_counts().index[0], inplace=True)\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_mean = missing_values(df_raw, 0.65, 0.65, \"mean\")\n",
    "df_median = missing_values(df_raw, 0.65, 0.65, \"median\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>depressed</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_ethnicity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#_ppl_household</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 missing  proportion\n",
       "depressed              0         0.0\n",
       "race_ethnicity         0         0.0\n",
       "#_ppl_household        0         0.0\n",
       "age                    0         0.0\n",
       "gender                 0         0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_data = nan_helper(df_raw)\n",
    "nan_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Preliminary Models Compare Imputation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "# now you can import normally from ensemble\n",
    "#from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to compare different types of imputation and their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depressed</th>\n",
       "      <th>race_ethnicity</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>#_ppl_household</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>annual_HI</th>\n",
       "      <th>caffeine</th>\n",
       "      <th>doc_diabetes</th>\n",
       "      <th>...</th>\n",
       "      <th>systolic_BP</th>\n",
       "      <th>diastolic_BP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>waist_C</th>\n",
       "      <th>#meals_fast_food</th>\n",
       "      <th>min_sedetary</th>\n",
       "      <th>doc_HPV</th>\n",
       "      <th>bone_mineral_density</th>\n",
       "      <th>difficult_hearing</th>\n",
       "      <th>doc_COPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>144.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>30.90</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>398.557696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.600000e+02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>138.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>24.74</td>\n",
       "      <td>96.5</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>384.781692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.420000e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>130.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30.63</td>\n",
       "      <td>117.1</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>382.287784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29.45</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>387.805700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>22.57</td>\n",
       "      <td>84.2</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>409.963013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   depressed  race_ethnicity  edu_level  #_ppl_household   age  gender  \\\n",
       "0        0.0             4.0        4.0              4.0  44.0     2.0   \n",
       "1        0.0             3.0        5.0              2.0  70.0     1.0   \n",
       "2        0.0             3.0        3.0              2.0  73.0     1.0   \n",
       "3        0.0             2.0        4.0              3.0  18.0     2.0   \n",
       "4        0.0             3.0        4.0              3.0  19.0     1.0   \n",
       "\n",
       "   marital_status  annual_HI      caffeine  doc_diabetes  ...  systolic_BP  \\\n",
       "0             1.0       11.0  1.300000e+01           0.0  ...        144.0   \n",
       "1             1.0       11.0  2.600000e+02           1.0  ...        138.0   \n",
       "2             1.0        6.0  1.420000e+02           0.0  ...        130.0   \n",
       "3             5.0       11.0  5.397605e-79           0.0  ...        110.0   \n",
       "4             5.0       11.0  5.397605e-79           0.0  ...        108.0   \n",
       "\n",
       "   diastolic_BP    BMI  waist_C  #meals_fast_food  min_sedetary  doc_HPV  \\\n",
       "0          74.0  30.90     96.0          2.093681    398.557696      0.0   \n",
       "1          60.0  24.74     96.5          2.093681    384.781692      0.0   \n",
       "2          68.0  30.63    117.1          2.093681    382.287784      0.0   \n",
       "3          64.0  29.45     84.0          2.093681    387.805700      0.0   \n",
       "4          62.0  22.57     84.2          2.093681    409.963013      0.0   \n",
       "\n",
       "   bone_mineral_density  difficult_hearing  doc_COPD  \n",
       "0              0.845891                0.0       0.0  \n",
       "1              0.845891                0.0       0.0  \n",
       "2              0.845891                0.0       0.0  \n",
       "3              0.845891                0.0       0.0  \n",
       "4              0.845891                0.0       0.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the knn and mlp imputed data so we do not have to run the function every time\n",
    "# code for knn and mlp imputation can be found at https://github.com/csklaver/Capstone-Group6/tree/master/Code\n",
    "knn_df = pd.read_csv('df_progressive_knn.csv')\n",
    "knn_df.drop(['SEQN'],axis=1,inplace=True)\n",
    "knn_df.drop(['year'],axis=1,inplace=True)\n",
    "\n",
    "mlp_df = pd.read_csv('df_progressive_mlp.csv')\n",
    "mlp_df.drop(['SEQN'],axis=1,inplace=True)\n",
    "mlp_df.drop(['year'],axis=1,inplace=True)\n",
    "mlp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_data(df_cleaned, impute_strategy=None, cols_to_standardize=None):\n",
    "    \"\"\"\n",
    "    Impute Data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_cleaned : dataframe without identifiers\n",
    "    impute_strategy: mean, median, or progressive_knn/mlp imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe without missing values from chosen imputation method\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df = df_cleaned.copy()\n",
    "    if impute_strategy == 'mean':\n",
    "        df = missing_values(df, 0.75, 0.75, 'mean')\n",
    "    elif impute_strategy == 'median':\n",
    "        df = missing_values(df, 0.75, 0.75, 'mean')\n",
    "    elif impute_strategy == 'progressive_knn':\n",
    "        df = knn_df\n",
    "    elif impute_strategy == 'progressive_mlp':\n",
    "        df = mlp_df\n",
    "    else:\n",
    "        arr = SimpleImputer(missing_values=np.nan,strategy=impute_strategy).fit(\n",
    "          df.values).transform(df.values)\n",
    "        df = pd.DataFrame(data=arr, index=df.index.values, columns=df.columns.values)\n",
    "    \n",
    "    if cols_to_standardize != None:\n",
    "        cols_to_standardize = list(set(cols_to_standardize) & set(df.columns.values))\n",
    "        df[cols_to_standardize] = df[cols_to_standardize].astype('float')\n",
    "        df[cols_to_standardize] = pd.DataFrame(data=MinMaxScaler().fit(\n",
    "        df[cols_to_standardize]).transform(df[cols_to_standardize]), \n",
    "                                             index=df[cols_to_standardize].index.values,\n",
    "                                             columns=df[cols_to_standardize].columns.values)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9510   71]\n",
      " [ 704   63]]\n",
      "[[9510   71]\n",
      " [ 704   63]]\n",
      "[[9517   64]\n",
      " [ 703   64]]\n",
      "[[9506   75]\n",
      " [ 695   72]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.151005</td>\n",
       "      <td>0.139845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>0.151005</td>\n",
       "      <td>0.139845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>0.154264</td>\n",
       "      <td>0.143017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>0.178133</td>\n",
       "      <td>0.157549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  train score  test score\n",
       "0                mean     0.151005    0.139845\n",
       "1              median     0.151005    0.139845\n",
       "2     progressive_knn     0.154264    0.143017\n",
       "3     progressive_mlp     0.178133    0.157549"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# function for handling missing values \n",
    "# and fitting logistic regression on clean data\n",
    "def log_reg(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.33,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Logistic Regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: f1-score on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state, shuffle=True)\n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    lg = LogisticRegression().fit(X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    y_pred = lg.predict(X_test)\n",
    "    train_score = f1_score(y_train, lg.predict(X_train))\n",
    "    \n",
    "    test_score = f1_score(y_test, y_pred)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'model': lg,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "lg_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn','progressive_mlp']: \n",
    "    result = log_reg(df, impute_strategy=impute_strategy, cols_to_standardize=cont)\n",
    "    lg_results.append(result)\n",
    "\n",
    "# display logistic regression performance\n",
    "lg_results_df = pd.DataFrame(lg_results)\n",
    "lg_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9576    5]\n",
      " [ 761    6]]\n",
      "0.015424164524421595\n",
      "[0. 1.]\n",
      "[[9576    5]\n",
      " [ 761    6]]\n",
      "0.015424164524421595\n",
      "[0. 1.]\n",
      "[[9575    6]\n",
      " [ 764    3]]\n",
      "0.007731958762886597\n",
      "[0. 1.]\n",
      "[[9577    4]\n",
      " [ 762    5]]\n",
      "0.012886597938144331\n",
      "[0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.015424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.015424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.007732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.012887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  train score  test score\n",
       "0                mean     0.999368    0.015424\n",
       "1              median     0.999368    0.015424\n",
       "2     progressive_knn     0.999684    0.007732\n",
       "3     progressive_mlp     0.999684    0.012887"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for handling missing values \n",
    "# and fitting random forest on clean data\n",
    "def decision_tree(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.33,\n",
    "                        random_state=42): \n",
    "    \"\"\"\n",
    "    Decision Tree\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: F1-score on training and testing set\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state, shuffle=True)\n",
    "    \n",
    "    # feature matrix\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    dt = DecisionTreeClassifier(class_weight='balanced', random_state=42).fit(\n",
    "        X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    y_pred = rf.predict(X_test)\n",
    "    train_score = f1_score(y_train, dt.predict(X_train))\n",
    "    test_score = f1_score(y_test, dt.predict(X_test))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f1_score(y_test, y_pred))\n",
    "    print(np.unique(y_pred))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'model': dt,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "dt_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw.copy()\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:  \n",
    "    result = random_forest(df, impute_strategy=impute_strategy, cols_to_standardize=cont)\n",
    "    dt_results.append(result)\n",
    "\n",
    "# display random forest regression performance\n",
    "dt_results_df = pd.DataFrame(dt_results)\n",
    "dt_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9576    5]\n",
      " [ 761    6]]\n",
      "0.015424164524421595\n",
      "[0. 1.]\n",
      "[[9576    5]\n",
      " [ 761    6]]\n",
      "0.015424164524421595\n",
      "[0. 1.]\n",
      "[[9575    6]\n",
      " [ 764    3]]\n",
      "0.007731958762886597\n",
      "[0. 1.]\n",
      "[[9577    4]\n",
      " [ 762    5]]\n",
      "0.012886597938144331\n",
      "[0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.015424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>0.999368</td>\n",
       "      <td>0.015424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.007732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>0.999684</td>\n",
       "      <td>0.012887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  train score  test score\n",
       "0                mean     0.999368    0.015424\n",
       "1              median     0.999368    0.015424\n",
       "2     progressive_knn     0.999684    0.007732\n",
       "3     progressive_mlp     0.999684    0.012887"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for handling missing values \n",
    "# and fitting random forest on clean data\n",
    "def random_forest(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.33,\n",
    "                        random_state=42): \n",
    "    \"\"\"\n",
    "    Random Forest\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: F1-score on training and testing set\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state, shuffle=True)\n",
    "    \n",
    "    # feature matrix\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    rf = RandomForestClassifier(class_weight='balanced', random_state=42).fit(\n",
    "        X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    y_pred = rf.predict(X_test)\n",
    "    train_score = f1_score(y_train, rf.predict(X_train))\n",
    "    test_score = f1_score(y_test, rf.predict(X_test))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f1_score(y_test, y_pred))\n",
    "    print(np.unique(y_pred))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'model': rf,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "rf_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw.copy()\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:  \n",
    "    result = random_forest(df, impute_strategy=impute_strategy, cols_to_standardize=cont)\n",
    "    rf_results.append(result)\n",
    "\n",
    "# display random forest regression performance\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "rf_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9466  115]\n",
      " [ 678   89]]\n",
      "0.18331616889804328\n",
      "[0. 1.]\n",
      "[[9466  115]\n",
      " [ 678   89]]\n",
      "0.18331616889804328\n",
      "[0. 1.]\n",
      "[[9480  101]\n",
      " [ 690   77]]\n",
      "0.16296296296296295\n",
      "[0. 1.]\n",
      "[[9469  112]\n",
      " [ 678   89]]\n",
      "0.18388429752066118\n",
      "[0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.799547</td>\n",
       "      <td>0.183316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>0.799547</td>\n",
       "      <td>0.183316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>0.837550</td>\n",
       "      <td>0.162963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>0.842836</td>\n",
       "      <td>0.183884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  train score  test score\n",
       "0                mean     0.799547    0.183316\n",
       "1              median     0.799547    0.183316\n",
       "2     progressive_knn     0.837550    0.162963\n",
       "3     progressive_mlp     0.842836    0.183884"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for handling missing values \n",
    "# and fitting random forest on clean data\n",
    "def xgboost(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.33,\n",
    "                        random_state=42): \n",
    "    \"\"\"\n",
    "    XGBoost\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: F1-score on training and testing set\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state, shuffle=True)\n",
    "    \n",
    "    # feature matrix\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    xgbc = XGBClassifier(random_state=42).fit(\n",
    "        X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    y_pred = xgbc.predict(X_test)\n",
    "    train_score = f1_score(y_train, xgbc.predict(X_train))\n",
    "    test_score = f1_score(y_test, y_pred)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(f1_score(y_test, y_pred))\n",
    "    print(np.unique(y_pred))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'model': xgbc,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "xgbc_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw.copy()\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:  \n",
    "    result = xgboost(df, impute_strategy=impute_strategy, cols_to_standardize=cont)\n",
    "    xgbc_results.append(result)\n",
    "\n",
    "# display random forest regression performance\n",
    "xgbc_results_df = pd.DataFrame(xgbc_results)\n",
    "xgbc_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9489   92]\n",
      " [ 724   43]]\n",
      "[[9489   92]\n",
      " [ 724   43]]\n",
      "[[9459  122]\n",
      " [ 723   44]]\n",
      "[[9481  100]\n",
      " [ 724   43]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.342289</td>\n",
       "      <td>0.095344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>0.342289</td>\n",
       "      <td>0.095344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>0.351190</td>\n",
       "      <td>0.094319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>0.359961</td>\n",
       "      <td>0.094505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  train score  test score\n",
       "0                mean     0.342289    0.095344\n",
       "1              median     0.342289    0.095344\n",
       "2     progressive_knn     0.351190    0.094319\n",
       "3     progressive_mlp     0.359961    0.094505"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for handling missing values \n",
    "# and fitting knn on clean data\n",
    "def knn_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.33,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: F1-score on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    knn = KNeighborsClassifier(n_neighbors=3, p=2, metric='minkowski').fit(\n",
    "        X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    y_pred = knn.predict(X_test)\n",
    "    train_score = f1_score(y_train, knn.predict(X_train))\n",
    "    test_score = f1_score(y_test, y_pred)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'model': knn,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "knn_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']: \n",
    "    result = knn_model(df, impute_strategy=impute_strategy, cols_to_standardize=cont)\n",
    "    knn_results.append(result)\n",
    "\n",
    "# display knn performance\n",
    "knn_results_df = pd.DataFrame(knn_results)\n",
    "knn_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7945 1636]\n",
      " [ 428  339]]\n",
      "[[7945 1636]\n",
      " [ 428  339]]\n",
      "[[7944 1637]\n",
      " [ 430  337]]\n",
      "[[7958 1623]\n",
      " [ 431  336]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.268444</td>\n",
       "      <td>0.247265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>0.268444</td>\n",
       "      <td>0.247265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>0.265976</td>\n",
       "      <td>0.245896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>0.267815</td>\n",
       "      <td>0.246515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  train score  test score\n",
       "0                mean     0.268444    0.247265\n",
       "1              median     0.268444    0.247265\n",
       "2     progressive_knn     0.265976    0.245896\n",
       "3     progressive_mlp     0.267815    0.246515"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def NB_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.33,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Naive Bayes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: F1-score on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state, shuffle=True)\n",
    "    \n",
    "    # feature matrix\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    nbc = GaussianNB().fit(\n",
    "        X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    y_pred = nbc.predict(X_test)\n",
    "    train_score = f1_score(y_train, nbc.predict(X_train))\n",
    "    test_score = f1_score(y_test, y_pred)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'model': nbc,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "nbc_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit nb for each imputation strategy\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:  \n",
    "    result = NB_model(df, impute_strategy=impute_strategy, cols_to_standardize=cont)\n",
    "    nbc_results.append(result)\n",
    "\n",
    "# display nb performance\n",
    "nbc_results_df = pd.DataFrame(nbc_results)\n",
    "nbc_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9512   69]\n",
      " [ 715   52]]\n",
      "[[9512   69]\n",
      " [ 715   52]]\n",
      "[[9569   12]\n",
      " [ 755   12]]\n",
      "[[8662  919]\n",
      " [ 490  277]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.134287</td>\n",
       "      <td>0.117117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>0.134287</td>\n",
       "      <td>0.117117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>0.030864</td>\n",
       "      <td>0.030341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>0.328172</td>\n",
       "      <td>0.282221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  train score  test score\n",
       "0                mean     0.134287    0.117117\n",
       "1              median     0.134287    0.117117\n",
       "2     progressive_knn     0.030864    0.030341\n",
       "3     progressive_mlp     0.328172    0.282221"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "def ppn_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.33,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Simple Perceptron Model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: Accuracy on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state, shuffle=True)\n",
    "\n",
    "    \n",
    "    # feature matrix\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    ppn = Perceptron(max_iter=40, eta0=0.1, random_state=0).fit(X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    train_score = f1_score(y_train, ppn.predict(X_train))\n",
    "    test_score = f1_score(y_test, ppn.predict(X_test))\n",
    "    y_pred = ppn.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'model': ppn,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "ppn_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit mlp for each imputation strategy\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:   \n",
    "    result = ppn_model(df, impute_strategy=impute_strategy, cols_to_standardize=cont)\n",
    "    ppn_results.append(result)\n",
    "\n",
    "# display mlp performance\n",
    "ppn_results_df = pd.DataFrame(ppn_results)\n",
    "ppn_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20068 samples, validate on 5017 samples\n",
      "Epoch 1/10\n",
      "20068/20068 [==============================] - 7s 350us/step - loss: 0.2495 - accuracy: 0.9250 - precision: 0.9250 - recall: 0.9250 - val_loss: 0.2617 - val_accuracy: 0.9199 - val_precision: 0.9199 - val_recall: 0.9199\n",
      "Epoch 2/10\n",
      "20068/20068 [==============================] - 6s 292us/step - loss: 0.2305 - accuracy: 0.9261 - precision: 0.9261 - recall: 0.9261 - val_loss: 0.2368 - val_accuracy: 0.9199 - val_precision: 0.9199 - val_recall: 0.9199\n",
      "Epoch 3/10\n",
      "20068/20068 [==============================] - 5s 266us/step - loss: 0.2268 - accuracy: 0.9269 - precision: 0.9269 - recall: 0.9269 - val_loss: 0.2358 - val_accuracy: 0.9221 - val_precision: 0.9221 - val_recall: 0.9221\n",
      "Epoch 4/10\n",
      "20068/20068 [==============================] - 7s 350us/step - loss: 0.2234 - accuracy: 0.9270 - precision: 0.9270 - recall: 0.9270 - val_loss: 0.2404 - val_accuracy: 0.9203 - val_precision: 0.9203 - val_recall: 0.9203\n",
      "Epoch 5/10\n",
      "20068/20068 [==============================] - 6s 308us/step - loss: 0.2222 - accuracy: 0.9280 - precision: 0.9280 - recall: 0.9280 - val_loss: 0.2360 - val_accuracy: 0.9211 - val_precision: 0.9211 - val_recall: 0.9211\n",
      "Epoch 6/10\n",
      "20068/20068 [==============================] - 6s 294us/step - loss: 0.2197 - accuracy: 0.9282 - precision: 0.9282 - recall: 0.9282 - val_loss: 0.2337 - val_accuracy: 0.9231 - val_precision: 0.9231 - val_recall: 0.9231\n",
      "Epoch 7/10\n",
      "20068/20068 [==============================] - 5s 252us/step - loss: 0.2173 - accuracy: 0.9290 - precision: 0.9290 - recall: 0.9290 - val_loss: 0.2369 - val_accuracy: 0.9225 - val_precision: 0.9225 - val_recall: 0.9225\n",
      "Epoch 8/10\n",
      "20068/20068 [==============================] - 6s 287us/step - loss: 0.2165 - accuracy: 0.9292 - precision: 0.9292 - recall: 0.9292 - val_loss: 0.2364 - val_accuracy: 0.9217 - val_precision: 0.9217 - val_recall: 0.9217\n",
      "Epoch 9/10\n",
      "20068/20068 [==============================] - 6s 288us/step - loss: 0.2144 - accuracy: 0.9294 - precision: 0.9294 - recall: 0.9294 - val_loss: 0.2390 - val_accuracy: 0.9215 - val_precision: 0.9215 - val_recall: 0.9215\n",
      "Epoch 10/10\n",
      "20068/20068 [==============================] - 6s 283us/step - loss: 0.2132 - accuracy: 0.9299 - precision: 0.9299 - recall: 0.9299 - val_loss: 0.2390 - val_accuracy: 0.9209 - val_precision: 0.9209 - val_recall: 0.9209\n",
      "Test loss: 0.23966926142421305\n",
      "Test accuracy: 0.9188456535339355\n",
      "Test precision: 0.9188456535339355\n",
      "Test recall: 0.9188456535339355\n",
      "F1-score 0.23458646616541348\n",
      "Train on 20068 samples, validate on 5017 samples\n",
      "Epoch 1/10\n",
      "20068/20068 [==============================] - 7s 347us/step - loss: 0.2523 - accuracy: 0.9239 - precision: 0.9239 - recall: 0.9239 - val_loss: 0.2495 - val_accuracy: 0.9199 - val_precision: 0.9199 - val_recall: 0.9199\n",
      "Epoch 2/10\n",
      "20068/20068 [==============================] - 5s 265us/step - loss: 0.2318 - accuracy: 0.9269 - precision: 0.9269 - recall: 0.9269 - val_loss: 0.2377 - val_accuracy: 0.9201 - val_precision: 0.9201 - val_recall: 0.9201\n",
      "Epoch 3/10\n",
      "20068/20068 [==============================] - 5s 266us/step - loss: 0.2247 - accuracy: 0.9271 - precision: 0.9271 - recall: 0.9271 - val_loss: 0.2345 - val_accuracy: 0.9201 - val_precision: 0.9201 - val_recall: 0.9201\n",
      "Epoch 4/10\n",
      "20068/20068 [==============================] - 5s 242us/step - loss: 0.2222 - accuracy: 0.9281 - precision: 0.9281 - recall: 0.9281 - val_loss: 0.2613 - val_accuracy: 0.9209 - val_precision: 0.9209 - val_recall: 0.9209\n",
      "Epoch 5/10\n",
      "20068/20068 [==============================] - 5s 255us/step - loss: 0.2211 - accuracy: 0.9279 - precision: 0.9279 - recall: 0.9279 - val_loss: 0.2368 - val_accuracy: 0.9207 - val_precision: 0.9207 - val_recall: 0.9207\n",
      "Epoch 6/10\n",
      "20068/20068 [==============================] - 6s 293us/step - loss: 0.2208 - accuracy: 0.9279 - precision: 0.9279 - recall: 0.9279 - val_loss: 0.2348 - val_accuracy: 0.9209 - val_precision: 0.9209 - val_recall: 0.9209\n",
      "Epoch 7/10\n",
      "20068/20068 [==============================] - 6s 284us/step - loss: 0.2173 - accuracy: 0.9284 - precision: 0.9284 - recall: 0.9284 - val_loss: 0.2448 - val_accuracy: 0.9209 - val_precision: 0.9209 - val_recall: 0.9209\n",
      "Epoch 8/10\n",
      "20068/20068 [==============================] - 5s 239us/step - loss: 0.2177 - accuracy: 0.9285 - precision: 0.9285 - recall: 0.9285 - val_loss: 0.2439 - val_accuracy: 0.9197 - val_precision: 0.9197 - val_recall: 0.9197\n",
      "Epoch 9/10\n",
      "20068/20068 [==============================] - 6s 278us/step - loss: 0.2153 - accuracy: 0.9292 - precision: 0.9292 - recall: 0.9292 - val_loss: 0.2501 - val_accuracy: 0.9203 - val_precision: 0.9203 - val_recall: 0.9203\n",
      "Epoch 10/10\n",
      "20068/20068 [==============================] - 6s 276us/step - loss: 0.2136 - accuracy: 0.9294 - precision: 0.9294 - recall: 0.9294 - val_loss: 0.2349 - val_accuracy: 0.9217 - val_precision: 0.9217 - val_recall: 0.9217\n",
      "Test loss: 0.23124232487182836\n",
      "Test accuracy: 0.9226722121238708\n",
      "Test precision: 0.9226722121238708\n",
      "Test recall: 0.9226722121238708\n",
      "F1-score 0.08317580340264649\n",
      "Train on 20068 samples, validate on 5017 samples\n",
      "Epoch 1/10\n",
      "20068/20068 [==============================] - 7s 343us/step - loss: 0.2536 - accuracy: 0.9231 - precision: 0.9231 - recall: 0.9231 - val_loss: 0.2822 - val_accuracy: 0.9197 - val_precision: 0.9197 - val_recall: 0.9197\n",
      "Epoch 2/10\n",
      "20068/20068 [==============================] - 7s 357us/step - loss: 0.2319 - accuracy: 0.9260 - precision: 0.9260 - recall: 0.9260 - val_loss: 0.2431 - val_accuracy: 0.9205 - val_precision: 0.9205 - val_recall: 0.9205\n",
      "Epoch 3/10\n",
      "20068/20068 [==============================] - 7s 343us/step - loss: 0.2262 - accuracy: 0.9278 - precision: 0.9278 - recall: 0.9278 - val_loss: 0.2408 - val_accuracy: 0.9203 - val_precision: 0.9203 - val_recall: 0.9203\n",
      "Epoch 4/10\n",
      "20068/20068 [==============================] - 6s 322us/step - loss: 0.2244 - accuracy: 0.9277 - precision: 0.9277 - recall: 0.9277 - val_loss: 0.2401 - val_accuracy: 0.9199 - val_precision: 0.9199 - val_recall: 0.9199\n",
      "Epoch 5/10\n",
      "20068/20068 [==============================] - 6s 304us/step - loss: 0.2207 - accuracy: 0.9272 - precision: 0.9272 - recall: 0.9272 - val_loss: 0.2370 - val_accuracy: 0.9223 - val_precision: 0.9223 - val_recall: 0.9223\n",
      "Epoch 6/10\n",
      "20068/20068 [==============================] - 5s 263us/step - loss: 0.2197 - accuracy: 0.9280 - precision: 0.9280 - recall: 0.9280 - val_loss: 0.2448 - val_accuracy: 0.9207 - val_precision: 0.9207 - val_recall: 0.9207\n",
      "Epoch 7/10\n",
      "20068/20068 [==============================] - 5s 243us/step - loss: 0.2178 - accuracy: 0.9289 - precision: 0.9289 - recall: 0.9289 - val_loss: 0.2380 - val_accuracy: 0.9209 - val_precision: 0.9209 - val_recall: 0.9209\n",
      "Epoch 8/10\n",
      "20068/20068 [==============================] - 5s 272us/step - loss: 0.2158 - accuracy: 0.9288 - precision: 0.9288 - recall: 0.9288 - val_loss: 0.2557 - val_accuracy: 0.9173 - val_precision: 0.9173 - val_recall: 0.9173\n",
      "Epoch 9/10\n",
      "20068/20068 [==============================] - 5s 240us/step - loss: 0.2142 - accuracy: 0.9286 - precision: 0.9286 - recall: 0.9286 - val_loss: 0.2383 - val_accuracy: 0.9215 - val_precision: 0.9215 - val_recall: 0.9215\n",
      "Epoch 10/10\n",
      "20068/20068 [==============================] - 4s 216us/step - loss: 0.2141 - accuracy: 0.9309 - precision: 0.9309 - recall: 0.9309 - val_loss: 0.2405 - val_accuracy: 0.9209 - val_precision: 0.9209 - val_recall: 0.9209\n",
      "Test loss: 0.23598425474245938\n",
      "Test accuracy: 0.9239476919174194\n",
      "Test precision: 0.9239476919174194\n",
      "Test recall: 0.9239476919174194\n",
      "F1-score 0.1675392670157068\n",
      "Train on 20068 samples, validate on 5017 samples\n",
      "Epoch 1/10\n",
      "20068/20068 [==============================] - 8s 405us/step - loss: 0.2483 - accuracy: 0.9263 - precision: 0.9263 - recall: 0.9263 - val_loss: 0.2386 - val_accuracy: 0.9215 - val_precision: 0.9215 - val_recall: 0.9215\n",
      "Epoch 2/10\n",
      "20068/20068 [==============================] - 6s 279us/step - loss: 0.2289 - accuracy: 0.9267 - precision: 0.9267 - recall: 0.9267 - val_loss: 0.2466 - val_accuracy: 0.9205 - val_precision: 0.9205 - val_recall: 0.9205\n",
      "Epoch 3/10\n",
      "20068/20068 [==============================] - 6s 286us/step - loss: 0.2261 - accuracy: 0.9276 - precision: 0.9276 - recall: 0.9276 - val_loss: 0.2463 - val_accuracy: 0.9235 - val_precision: 0.9235 - val_recall: 0.9235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "20068/20068 [==============================] - 7s 326us/step - loss: 0.2230 - accuracy: 0.9282 - precision: 0.9282 - recall: 0.9282 - val_loss: 0.2369 - val_accuracy: 0.9205 - val_precision: 0.9205 - val_recall: 0.9205\n",
      "Epoch 5/10\n",
      "20068/20068 [==============================] - 6s 293us/step - loss: 0.2198 - accuracy: 0.9286 - precision: 0.9286 - recall: 0.9286 - val_loss: 0.2377 - val_accuracy: 0.9209 - val_precision: 0.9209 - val_recall: 0.9209\n",
      "Epoch 6/10\n",
      "20068/20068 [==============================] - 5s 267us/step - loss: 0.2200 - accuracy: 0.9271 - precision: 0.9271 - recall: 0.9271 - val_loss: 0.2354 - val_accuracy: 0.9227 - val_precision: 0.9227 - val_recall: 0.9227\n",
      "Epoch 7/10\n",
      "20068/20068 [==============================] - 5s 254us/step - loss: 0.2169 - accuracy: 0.9289 - precision: 0.9289 - recall: 0.9289 - val_loss: 0.2428 - val_accuracy: 0.9245 - val_precision: 0.9245 - val_recall: 0.9245\n",
      "Epoch 8/10\n",
      "20068/20068 [==============================] - 5s 267us/step - loss: 0.2161 - accuracy: 0.9288 - precision: 0.9288 - recall: 0.9288 - val_loss: 0.2412 - val_accuracy: 0.9217 - val_precision: 0.9217 - val_recall: 0.9217\n",
      "Epoch 9/10\n",
      "20068/20068 [==============================] - 6s 291us/step - loss: 0.2158 - accuracy: 0.9289 - precision: 0.9289 - recall: 0.9289 - val_loss: 0.2377 - val_accuracy: 0.9219 - val_precision: 0.9219 - val_recall: 0.9219\n",
      "Epoch 10/10\n",
      "20068/20068 [==============================] - 6s 298us/step - loss: 0.2149 - accuracy: 0.9292 - precision: 0.9292 - recall: 0.9292 - val_loss: 0.2356 - val_accuracy: 0.9223 - val_precision: 0.9223 - val_recall: 0.9223\n",
      "Test loss: 0.23311173714393255\n",
      "Test accuracy: 0.9234693646430969\n",
      "Test precision: 0.9234693646430969\n",
      "Test recall: 0.9234693646430969\n",
      "F1-score 0.14893617021276598\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>standardized</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>Test F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>True</td>\n",
       "      <td>0.239669</td>\n",
       "      <td>0.918846</td>\n",
       "      <td>0.234586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>median</td>\n",
       "      <td>True</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.922672</td>\n",
       "      <td>0.083176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>True</td>\n",
       "      <td>0.235984</td>\n",
       "      <td>0.923948</td>\n",
       "      <td>0.167539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>True</td>\n",
       "      <td>0.233112</td>\n",
       "      <td>0.923469</td>\n",
       "      <td>0.148936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  standardized  Test loss  Test accuracy  Test F1-score\n",
       "0                mean          True   0.239669       0.918846       0.234586\n",
       "1              median          True   0.231242       0.922672       0.083176\n",
       "2     progressive_knn          True   0.235984       0.923948       0.167539\n",
       "3     progressive_mlp          True   0.233112       0.923469       0.148936"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "def keras_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Keras MLP\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: Accuracy on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = 128\n",
    "    epochs = 10\n",
    "\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state, shuffle=True)\n",
    "    \n",
    "    # note which predictor columns were dropped or kept\n",
    "    kept_columns = df_imputed.columns.difference(['depressed'])\n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, 2)\n",
    "    y_test = keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(len(kept_columns),)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy', keras.metrics.Precision(name='precision'),\n",
    "                                        keras.metrics.Recall(name='recall')])\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.2)\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    # model evaulation\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print('Test precision:',score[2])\n",
    "    print('Test recall:',score[3])\n",
    "    f1 = f1_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print('F1-score', f1)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'standardized': cols_to_standardize!=None,\n",
    "        'model': keras_model,\n",
    "        'Test loss' :  score[0],\n",
    "        'Test accuracy' : score[1],\n",
    "        'Test F1-score': f1\n",
    "#         'Test precision': score[2],\n",
    "#         'Test recall': score[3]\n",
    "        \n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "keras_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit keras mlp for each imputation strategy\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']: \n",
    "    result = keras_model(df, impute_strategy=impute_strategy, cols_to_standardize=cont)\n",
    "    keras_results.append(result)\n",
    "\n",
    "# display keras mlp performance\n",
    "keras_results_df = pd.DataFrame(keras_results)\n",
    "keras_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an imbalanced target variable of depression (7.28% positive), these models are exposing the issue with imbalanced data. Accuracy for training and testing is high, but F1-score is extremely low. The simple keras neural network is not robust enough to detect the positive depression instances. Next, build a deeper model as well as explore resampling techniques and k-fold cross-validation to get an accurate model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "78nn7ejt4AOI",
    "IE9wOLk84AOM",
    "ei8r0bJP4AOU",
    "_qlzJj8P4AOW",
    "qzgPDkxM4AOg",
    "aFZrDH4G4AOn",
    "n1wNsrxv4AOr",
    "FAnUmBDm4AOv"
   ],
   "name": "case_study_4_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
