{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTiWqATn4ANO"
   },
   "source": [
    "<b>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Data Science Capstone, Summer 2020\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Predicting Depression\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Data Science, Columbian College of Arts & Sciences, George Washington University\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Author: Caroline Sklaver\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jWmYBTOwgNs-",
    "outputId": "3886a80c-7341-4485-f2c4-9cfd4b1bb046"
   },
   "outputs": [],
   "source": [
    "path = ('/Users/carolinesklaver/Desktop/Capstone/NHANES/data/csv_data/')\n",
    "\n",
    "import os\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MUl4k83e4ANR"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBRVH9SB4ANb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eOpQpPu4ANk"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "\n",
    "df_raw = pd.read_csv('df_raw_v2.csv')\n",
    "\n",
    "# bring year and target col to the beginning of df\n",
    "year = df_raw.pop('year')\n",
    "df_raw.insert(1, 'year', year)\n",
    "\n",
    "dep = df_raw.pop('depressed')\n",
    "df_raw.insert(2, 'depressed', dep)\n",
    "\n",
    "\n",
    "\n",
    "# drop marijuana use\n",
    "df_raw.drop(['used_marijuana'],axis=1, inplace=True)\n",
    "# help!\n",
    "df_raw.drop(['year'],axis=1, inplace=True)\n",
    "\n",
    "df_raw.drop(['SEQN'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continuous features\n",
    "cont = ['#_ppl_household', 'age', 'triglyceride','caffeine', 'lifetime_partners',\n",
    "       'glycohemoglobin', 'CRP', 'tot_cholesterol','systolic_BP','diastolic_BP', 'BMI', 'waist_C', '#meals_fast_food',\n",
    "       'min_sedetary', 'bone_mineral_density']\n",
    "\n",
    "# categorical features\n",
    "cat = ['race_ethnicity', 'edu_level', 'gender', 'marital_status', 'annual_HI',\n",
    "       'doc_diabetes', 'how_healthy_diet', 'used_CMH',\n",
    "       'health_insurance', 'doc_asthma', 'doc_overweight', 'doc_arthritis',\n",
    "       'doc_CHF', 'doc_CHD', 'doc_heart_attack', 'doc_stroke',\n",
    "       'doc_chronic_bronchitis', 'doc_liver_condition', 'doc_thyroid_problem',\n",
    "       'doc_cancer', 'difficult_seeing', 'doc_kidney', 'broken_hip',\n",
    "       'doc_osteoporosis', 'vigorous_activity', 'moderate_activity',\n",
    "       'doc_sleeping_disorder', 'smoker', 'sexual_orientation',\n",
    "       'alcoholic','herpes_2', 'HIV', 'doc_HPV','difficult_hearing', 'doc_COPD']\n",
    "\n",
    "# target binary feature\n",
    "target = 'depressed'\n",
    "\n",
    "# multi-class features\n",
    "cat_encode = ['race_ethnicity', 'edu_level', 'gender', 'marital_status', 'annual_HI','how_healthy_diet',\n",
    "              'sexual_orientation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6onKpDeL4ANn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def nan_helper(df):\n",
    "    \"\"\"\n",
    "    The NaN helper\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe of variables with NaN (index), \n",
    "    raw number missing, and their proportion\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # get the raw number of missing values & sort\n",
    "    missing = df.isnull().sum().sort_values(ascending=True)\n",
    "    \n",
    "    # get the proportion of missing values (%)\n",
    "    proportion = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=True)\n",
    "    \n",
    "    # create table of missing data\n",
    "    nan_data = pd.concat([missing, proportion], axis=1, keys=['missing', 'proportion'])\n",
    "    \n",
    "    return nan_data\n",
    "\n",
    "\n",
    "def missing_values(df, threshold_col, threshold_row, impute_type):\n",
    "    \"\"\"\n",
    "    Handle Missing Values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    threshold_col: the proportion of missing values at which  to drop whole column\n",
    "    threshold_row: the proportion of missing values at which to drop rows\n",
    "    impute_type: mean or median imputation for continuous variables\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe without missing values\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Dropping Cols and Rows\n",
    "    # call NaN helper function\n",
    "    df_nan = nan_helper(df)\n",
    "        \n",
    "    # drop columns with higher proportion missing than threshold col\n",
    "    df = df.drop((df_nan[df_nan['proportion'] > threshold_col]).index,1)\n",
    "    \n",
    "    # drop rows with higher proportion missing than threshold row\n",
    "    df_nan_2 = df_nan[df_nan['proportion']>threshold_row]\n",
    "    df = df.dropna(subset=np.intersect1d(df_nan_2.index, df.columns),\n",
    "                           inplace=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Imputing values\n",
    "    # Impute continuous variables with mean \n",
    "    if impute_type == 'mean':\n",
    "        for col in cont:\n",
    "            if col in df.columns:\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "    # Impute continuous variables with median\n",
    "    elif impute_type == 'median':\n",
    "        for col in cont:\n",
    "            if col in df.columns:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    \n",
    "    # Impute categorical variables with most frequent/mode\n",
    "    for col in cat:\n",
    "        if col in df.columns:\n",
    "            df[col].fillna(df[col].value_counts().index[0], inplace=True)\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_mean = missing_values(df_raw, 0.65, 0.65, \"mean\")\n",
    "df_median = missing_values(df_raw, 0.65, 0.65, \"median\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>depressed</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_ethnicity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#_ppl_household</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 missing  proportion\n",
       "depressed              0         0.0\n",
       "race_ethnicity         0         0.0\n",
       "#_ppl_household        0         0.0\n",
       "age                    0         0.0\n",
       "gender                 0         0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_data = nan_helper(df_raw)\n",
    "nan_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7dxOrdj4AOm"
   },
   "source": [
    "## Encoding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFZrDH4G4AOn"
   },
   "source": [
    "### Combining the training, validation and testing data\n",
    "The code below shows how to combine the training, validation and testing data (using pandas concat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "id": "_-iTUGvJ4AOn",
    "outputId": "12dd3838-5fbb-4619-b655-c7334a63ea47",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Combine df_train, df_valid and df_test\n",
    "# df = pd.concat([df_train, df_valid, df_test], sort=False)\n",
    "\n",
    "# # Print the unique dtype of variables in df\n",
    "# pd.DataFrame(df.dtypes.unique(), columns=['dtype'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAnUmBDm4AOv"
   },
   "source": [
    "### Encoding the categorical features\n",
    "The code below shows how to encode the categorical features in the combined data (using pandas.get\\_dummies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "ff2r2dma4AOv",
    "outputId": "dea7897f-51f7-4e72-bab6-6fb24aaff9c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # One-hot-encode the categorical features in the combined data\n",
    "# df = pd.get_dummies(df, columns=cat_encode)\n",
    "\n",
    "# # Print the first 5 rows of df\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlO7g6EO4AOx"
   },
   "source": [
    "### Encoding the categorical target\n",
    "The code below shows how to encode the categorical target in the combined data (using sklearn.LabelEncoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "NrKQYywn4AOx",
    "outputId": "cc651420-cfc6-441e-d1f0-ee454bc8e6a1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # The LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "\n",
    "# # Encode the categorical target in the combined data\n",
    "# df[target] = le.fit_transform(df[target].astype(str))\n",
    "\n",
    "# # Print the first 5 rows of df\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0OW9egQ4AOy"
   },
   "source": [
    "### Separating the training, validation and testing data\n",
    "The code below shows how to separate the training, validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xj2ygps_4AOz"
   },
   "outputs": [],
   "source": [
    "# # Separating the training data\n",
    "# df_train = df.iloc[:df_train.shape[0], :].copy(deep=True)\n",
    "\n",
    "# # Separating the validation data\n",
    "# df_valid = df.iloc[df_train.shape[0]:df_train.shape[0] + df_valid.shape[0], :].copy(deep=True)\n",
    "\n",
    "# # Separating the testing data\n",
    "# df_test = df.iloc[df_train.shape[0] + df_valid.shape[0]:, :].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "WlQzE2Yt4AO0",
    "outputId": "760bd041-aa66-4565-e55f-75630464f80f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Print the dimension of df_train\n",
    "# pd.DataFrame([[df_train.shape[0], df_train.shape[1]]], columns=['# rows', '# columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "KdNjOqRw4AO2",
    "outputId": "66c67d26-82c9-4ba9-987f-85993609818e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Print the dimension of df_valid\n",
    "# pd.DataFrame([[df_valid.shape[0], df_valid.shape[1]]], columns=['# rows', '# columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "1JKo7F_F4AO3",
    "outputId": "073e6629-98d6-442b-fc9e-95624f6f0cec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Print the dimension of df_test\n",
    "# pd.DataFrame([[df_test.shape[0], df_test.shape[1]]], columns=['# rows', '# columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZuMdgk54AO8"
   },
   "source": [
    "## Scaling the data\n",
    "The code below shows how to normalize the data (using sklearn MinMaxScaler). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7aKuH8ZY4AO9"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # The MinMaxScaler\n",
    "# mms = MinMaxScaler()\n",
    "\n",
    "# # Normalize the training data\n",
    "# X_train = mms.fit_transform(X_train)\n",
    "\n",
    "# # Normalize the validation data\n",
    "# X_valid = mms.transform(X_valid)\n",
    "\n",
    "# # Normalize the testing data\n",
    "# X_test = mms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "# now you can import normally from ensemble\n",
    "#from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# # perform training\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = rf.predict(X_test)\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SVC(kernel='linear', C=1.0, random_state=0)\n",
    "# s = svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# y_pred = s.predict(X_test)\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = KNeighborsClassifier(n_neighbors=1, p=2,metric='minkowski')\n",
    "# k = knn.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = k.predict(X_test)\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to compare different types of imputation and their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depressed</th>\n",
       "      <th>race_ethnicity</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>#_ppl_household</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>annual_HI</th>\n",
       "      <th>caffeine</th>\n",
       "      <th>doc_diabetes</th>\n",
       "      <th>...</th>\n",
       "      <th>systolic_BP</th>\n",
       "      <th>diastolic_BP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>waist_C</th>\n",
       "      <th>#meals_fast_food</th>\n",
       "      <th>min_sedetary</th>\n",
       "      <th>doc_HPV</th>\n",
       "      <th>bone_mineral_density</th>\n",
       "      <th>difficult_hearing</th>\n",
       "      <th>doc_COPD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>144.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>30.90</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>398.557696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.600000e+02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>138.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>24.74</td>\n",
       "      <td>96.5</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>384.781692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.420000e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>130.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30.63</td>\n",
       "      <td>117.1</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>382.287784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>29.45</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>387.805700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>22.57</td>\n",
       "      <td>84.2</td>\n",
       "      <td>2.093681</td>\n",
       "      <td>409.963013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.845891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   depressed  race_ethnicity  edu_level  #_ppl_household   age  gender  \\\n",
       "0        0.0             4.0        4.0              4.0  44.0     2.0   \n",
       "1        0.0             3.0        5.0              2.0  70.0     1.0   \n",
       "2        0.0             3.0        3.0              2.0  73.0     1.0   \n",
       "3        0.0             2.0        4.0              3.0  18.0     2.0   \n",
       "4        0.0             3.0        4.0              3.0  19.0     1.0   \n",
       "\n",
       "   marital_status  annual_HI      caffeine  doc_diabetes  ...  systolic_BP  \\\n",
       "0             1.0       11.0  1.300000e+01           0.0  ...        144.0   \n",
       "1             1.0       11.0  2.600000e+02           1.0  ...        138.0   \n",
       "2             1.0        6.0  1.420000e+02           0.0  ...        130.0   \n",
       "3             5.0       11.0  5.397605e-79           0.0  ...        110.0   \n",
       "4             5.0       11.0  5.397605e-79           0.0  ...        108.0   \n",
       "\n",
       "   diastolic_BP    BMI  waist_C  #meals_fast_food  min_sedetary  doc_HPV  \\\n",
       "0          74.0  30.90     96.0          2.093681    398.557696      0.0   \n",
       "1          60.0  24.74     96.5          2.093681    384.781692      0.0   \n",
       "2          68.0  30.63    117.1          2.093681    382.287784      0.0   \n",
       "3          64.0  29.45     84.0          2.093681    387.805700      0.0   \n",
       "4          62.0  22.57     84.2          2.093681    409.963013      0.0   \n",
       "\n",
       "   bone_mineral_density  difficult_hearing  doc_COPD  \n",
       "0              0.845891                0.0       0.0  \n",
       "1              0.845891                0.0       0.0  \n",
       "2              0.845891                0.0       0.0  \n",
       "3              0.845891                0.0       0.0  \n",
       "4              0.845891                0.0       0.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the knn imputed data so we do not have to run the function every time\n",
    "knn_df = pd.read_csv('df_progressive_knn.csv')\n",
    "knn_df.drop(['SEQN'],axis=1,inplace=True)\n",
    "knn_df.drop(['year'],axis=1,inplace=True)\n",
    "#knn_df.head()\n",
    "\n",
    "\n",
    "mlp_df = pd.read_csv('df_progressive_mlp.csv')\n",
    "mlp_df.drop(['SEQN'],axis=1,inplace=True)\n",
    "mlp_df.drop(['year'],axis=1,inplace=True)\n",
    "mlp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_data(df_cleaned, impute_strategy=None, cols_to_standardize=None):\n",
    "    \"\"\"\n",
    "    Impute Data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_cleaned : dataframe without identifiers\n",
    "    impute_strategy: mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    The dataframe without missing values from chosen imputation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df = df_cleaned.copy()\n",
    "    if impute_strategy == 'mean':\n",
    "        df = missing_values(df, 0.7, 0.7, 'mean')\n",
    "    elif impute_strategy == 'median':\n",
    "        df = missing_values(df, 0.7, 0.7, 'mean')\n",
    "    elif impute_strategy == 'progressive_knn':\n",
    "        df = knn_df\n",
    "    elif impute_strategy == 'progressive_mlp':\n",
    "        df = mlp_df\n",
    "    else:\n",
    "        arr = SimpleImputer(missing_values=np.nan,strategy=impute_strategy).fit(\n",
    "          df.values).transform(df.values)\n",
    "        df = pd.DataFrame(data=arr, index=df.index.values, columns=df.columns.values)\n",
    "    \n",
    "    if cols_to_standardize != None:\n",
    "        cols_to_standardize = list(set(cols_to_standardize) & set(df.columns.values))\n",
    "        df[cols_to_standardize] = df[cols_to_standardize].astype('float')\n",
    "        df[cols_to_standardize] = pd.DataFrame(data=MinMaxScaler().fit(\n",
    "        df[cols_to_standardize]).transform(df[cols_to_standardize]), \n",
    "                                             index=df[cols_to_standardize].index.values,\n",
    "                                             columns=df[cols_to_standardize].columns.values)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5800    2]\n",
      " [ 468    2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      5802\n",
      "         1.0       0.50      0.00      0.01       470\n",
      "\n",
      "    accuracy                           0.93      6272\n",
      "   macro avg       0.71      0.50      0.48      6272\n",
      "weighted avg       0.89      0.93      0.89      6272\n",
      "\n",
      "[0. 1.]\n",
      "Execution time: 4.581997180000144\n",
      "[[5800    2]\n",
      " [ 467    3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      5802\n",
      "         1.0       0.60      0.01      0.01       470\n",
      "\n",
      "    accuracy                           0.93      6272\n",
      "   macro avg       0.76      0.50      0.49      6272\n",
      "weighted avg       0.90      0.93      0.89      6272\n",
      "\n",
      "[0. 1.]\n",
      "Execution time: 4.532017135999922\n",
      "[[5800    2]\n",
      " [ 468    2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      5802\n",
      "         1.0       0.50      0.00      0.01       470\n",
      "\n",
      "    accuracy                           0.93      6272\n",
      "   macro avg       0.71      0.50      0.48      6272\n",
      "weighted avg       0.89      0.93      0.89      6272\n",
      "\n",
      "[0. 1.]\n",
      "Execution time: 5.185357879999174\n",
      "[[5800    2]\n",
      " [ 467    3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      5802\n",
      "         1.0       0.60      0.01      0.01       470\n",
      "\n",
      "    accuracy                           0.93      6272\n",
      "   macro avg       0.76      0.50      0.49      6272\n",
      "weighted avg       0.90      0.93      0.89      6272\n",
      "\n",
      "[0. 1.]\n",
      "Execution time: 4.916818732000138\n",
      "[[5799    3]\n",
      " [ 466    4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      5802\n",
      "         1.0       0.57      0.01      0.02       470\n",
      "\n",
      "    accuracy                           0.93      6272\n",
      "   macro avg       0.75      0.50      0.49      6272\n",
      "weighted avg       0.90      0.93      0.89      6272\n",
      "\n",
      "[0. 1.]\n",
      "Execution time: 5.4165746789994955\n",
      "[[5799    3]\n",
      " [ 466    4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      5802\n",
      "         1.0       0.57      0.01      0.02       470\n",
      "\n",
      "    accuracy                           0.93      6272\n",
      "   macro avg       0.75      0.50      0.49      6272\n",
      "weighted avg       0.90      0.93      0.89      6272\n",
      "\n",
      "[0. 1.]\n",
      "Execution time: 5.584785155000645\n",
      "[[5797    5]\n",
      " [ 468    2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      5802\n",
      "         1.0       0.29      0.00      0.01       470\n",
      "\n",
      "    accuracy                           0.92      6272\n",
      "   macro avg       0.61      0.50      0.48      6272\n",
      "weighted avg       0.88      0.92      0.89      6272\n",
      "\n",
      "[0. 1.]\n",
      "Execution time: 5.592290775000038\n",
      "[[5797    5]\n",
      " [ 465    5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      1.00      0.96      5802\n",
      "         1.0       0.50      0.01      0.02       470\n",
      "\n",
      "    accuracy                           0.93      6272\n",
      "   macro avg       0.71      0.50      0.49      6272\n",
      "weighted avg       0.89      0.93      0.89      6272\n",
      "\n",
      "[0. 1.]\n",
      "Execution time: 8.393830478000382\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>standardized</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>execution time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>False</td>\n",
       "      <td>0.99996</td>\n",
       "      <td>0.925064</td>\n",
       "      <td>4.581997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>True</td>\n",
       "      <td>0.99996</td>\n",
       "      <td>0.925223</td>\n",
       "      <td>4.532017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>median</td>\n",
       "      <td>False</td>\n",
       "      <td>0.99996</td>\n",
       "      <td>0.925064</td>\n",
       "      <td>5.185358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>median</td>\n",
       "      <td>True</td>\n",
       "      <td>0.99996</td>\n",
       "      <td>0.925223</td>\n",
       "      <td>4.916819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>False</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.925223</td>\n",
       "      <td>5.416575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.925223</td>\n",
       "      <td>5.584785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>False</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.924585</td>\n",
       "      <td>5.592291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>True</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.925064</td>\n",
       "      <td>8.393830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  standardized  train score  test score  \\\n",
       "0                mean         False      0.99996    0.925064   \n",
       "1                mean          True      0.99996    0.925223   \n",
       "2              median         False      0.99996    0.925064   \n",
       "3              median          True      0.99996    0.925223   \n",
       "4     progressive_knn         False      1.00000    0.925223   \n",
       "5     progressive_knn          True      1.00000    0.925223   \n",
       "6     progressive_mlp         False      1.00000    0.924585   \n",
       "7     progressive_mlp          True      1.00000    0.925064   \n",
       "\n",
       "   execution time (s)  \n",
       "0            4.581997  \n",
       "1            4.532017  \n",
       "2            5.185358  \n",
       "3            4.916819  \n",
       "4            5.416575  \n",
       "5            5.584785  \n",
       "6            5.592291  \n",
       "7            8.393830  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# function for handling missing values \n",
    "# and fitting random forest on clean data\n",
    "def random_forest(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42): \n",
    "    \"\"\"\n",
    "    Random Forest\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: Accuracy on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # store original columns\n",
    "    original_columns = data.columns.difference(['depressed'])\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state)\n",
    "    \n",
    "    # note which predictor columns were dropped or kept\n",
    "    kept_columns = df_imputed.columns.difference(['depressed'])\n",
    "    dropped_columns = original_columns.difference(df_imputed.columns)\n",
    "    original_columns = original_columns.difference(['depressed'])\n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    rf = RandomForestClassifier(class_weight='balanced', random_state=42).fit(\n",
    "        X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    train_score = accuracy_score(y_train, rf.predict(X_train))\n",
    "    test_score = accuracy_score(y_test, rf.predict(X_test))\n",
    "    duration = timer() - start\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(np.unique(y_pred))\n",
    "    #print(\"Classification rate on training data: {}\".format(train_score))\n",
    "    #print(\"Classification rate on test data: {}\".format(test_score))\n",
    "    print(\"Execution time: {}\".format(duration))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'standardized': cols_to_standardize!=None,\n",
    "        'model': rf,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "        'execution time (s)': duration\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "rf_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:\n",
    "    for cols in [None, cols_to_standardize]:   \n",
    "        result = random_forest(df, impute_strategy=impute_strategy, cols_to_standardize=cols)\n",
    "        rf_results.append(result)\n",
    "\n",
    "# display random forest regression performance\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "rf_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5709   93]\n",
      " [ 463    7]]\n",
      "Execution time: 10.90446405900002\n",
      "[[5737   65]\n",
      " [ 442   28]]\n",
      "Execution time: 34.70661648999976\n",
      "[[5709   93]\n",
      " [ 463    7]]\n",
      "Execution time: 6.706074951000119\n",
      "[[5737   65]\n",
      " [ 442   28]]\n",
      "Execution time: 30.20394973099974\n",
      "[[5740   62]\n",
      " [ 446   24]]\n",
      "Execution time: 25.22012957900006\n",
      "[[5740   62]\n",
      " [ 446   24]]\n",
      "Execution time: 31.889374418999978\n",
      "[[5733   69]\n",
      " [ 442   28]]\n",
      "Execution time: 23.235407035000208\n",
      "[[5733   69]\n",
      " [ 442   28]]\n",
      "Execution time: 23.003710847000548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>standardized</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>execution time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>False</td>\n",
       "      <td>0.931951</td>\n",
       "      <td>0.911352</td>\n",
       "      <td>10.904464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>True</td>\n",
       "      <td>0.938130</td>\n",
       "      <td>0.919165</td>\n",
       "      <td>34.706616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>median</td>\n",
       "      <td>False</td>\n",
       "      <td>0.931951</td>\n",
       "      <td>0.911352</td>\n",
       "      <td>6.706075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>median</td>\n",
       "      <td>True</td>\n",
       "      <td>0.938130</td>\n",
       "      <td>0.919165</td>\n",
       "      <td>30.203950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>False</td>\n",
       "      <td>0.937851</td>\n",
       "      <td>0.919005</td>\n",
       "      <td>25.220130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>True</td>\n",
       "      <td>0.937851</td>\n",
       "      <td>0.919005</td>\n",
       "      <td>31.889374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>False</td>\n",
       "      <td>0.938489</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>23.235407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>True</td>\n",
       "      <td>0.938489</td>\n",
       "      <td>0.918527</td>\n",
       "      <td>23.003711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  standardized  train score  test score  \\\n",
       "0                mean         False     0.931951    0.911352   \n",
       "1                mean          True     0.938130    0.919165   \n",
       "2              median         False     0.931951    0.911352   \n",
       "3              median          True     0.938130    0.919165   \n",
       "4     progressive_knn         False     0.937851    0.919005   \n",
       "5     progressive_knn          True     0.937851    0.919005   \n",
       "6     progressive_mlp         False     0.938489    0.918527   \n",
       "7     progressive_mlp          True     0.938489    0.918527   \n",
       "\n",
       "   execution time (s)  \n",
       "0           10.904464  \n",
       "1           34.706616  \n",
       "2            6.706075  \n",
       "3           30.203950  \n",
       "4           25.220130  \n",
       "5           31.889374  \n",
       "6           23.235407  \n",
       "7           23.003711  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for handling missing values \n",
    "# and fitting knn on clean data\n",
    "def knn_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: Accuracy on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # store original columns\n",
    "    original_columns = data.columns.difference(['depressed'])\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state)\n",
    "    \n",
    "    # note which predictor columns were dropped or kept\n",
    "    kept_columns = df_imputed.columns.difference(['depressed'])\n",
    "    dropped_columns = original_columns.difference(df_imputed.columns)\n",
    "    original_columns = original_columns.difference(['depressed'])\n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    knn = KNeighborsClassifier(n_neighbors=3, p=2,metric='minkowski').fit(\n",
    "        X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    train_score = accuracy_score(y_train, knn.predict(X_train))\n",
    "    test_score = accuracy_score(y_test, knn.predict(X_test))\n",
    "    duration = timer() - start\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "    #print(\"Classification rate on training data: {}\".format(train_score))\n",
    "    #print(\"Classification rate on test data: {}\".format(test_score))\n",
    "    print(\"Execution time: {}\".format(duration))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'standardized': cols_to_standardize!=None,\n",
    "        'model': knn,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "        'execution time (s)': duration\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "knn_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:\n",
    "    for cols in [None, cols_to_standardize]:   \n",
    "        result = knn_model(df, impute_strategy=impute_strategy, cols_to_standardize=cols)\n",
    "        knn_results.append(result)\n",
    "\n",
    "# display knn performance\n",
    "knn_results_df = pd.DataFrame(knn_results)\n",
    "knn_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4780 1022]\n",
      " [ 257  213]]\n",
      "Execution time: 0.2950997690004442\n",
      "[[4779 1023]\n",
      " [ 257  213]]\n",
      "Execution time: 0.1727525220003372\n",
      "[[4780 1022]\n",
      " [ 257  213]]\n",
      "Execution time: 0.17494639300002746\n",
      "[[4779 1023]\n",
      " [ 257  213]]\n",
      "Execution time: 0.18341273399983038\n",
      "[[4787 1015]\n",
      " [ 261  209]]\n",
      "Execution time: 0.0908743569998478\n",
      "[[4787 1015]\n",
      " [ 261  209]]\n",
      "Execution time: 0.1108400069997515\n",
      "[[4796 1006]\n",
      " [ 261  209]]\n",
      "Execution time: 0.07934491800006072\n",
      "[[4796 1006]\n",
      " [ 261  209]]\n",
      "Execution time: 0.12824778300000617\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>standardized</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>execution time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>False</td>\n",
       "      <td>0.803947</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.295100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>True</td>\n",
       "      <td>0.803867</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.172753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>median</td>\n",
       "      <td>False</td>\n",
       "      <td>0.803947</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.174946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>median</td>\n",
       "      <td>True</td>\n",
       "      <td>0.803867</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.183413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>False</td>\n",
       "      <td>0.803428</td>\n",
       "      <td>0.796556</td>\n",
       "      <td>0.090874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>True</td>\n",
       "      <td>0.803428</td>\n",
       "      <td>0.796556</td>\n",
       "      <td>0.110840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>False</td>\n",
       "      <td>0.807375</td>\n",
       "      <td>0.797991</td>\n",
       "      <td>0.079345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>True</td>\n",
       "      <td>0.807375</td>\n",
       "      <td>0.797991</td>\n",
       "      <td>0.128248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  standardized  train score  test score  \\\n",
       "0                mean         False     0.803947    0.796078   \n",
       "1                mean          True     0.803867    0.795918   \n",
       "2              median         False     0.803947    0.796078   \n",
       "3              median          True     0.803867    0.795918   \n",
       "4     progressive_knn         False     0.803428    0.796556   \n",
       "5     progressive_knn          True     0.803428    0.796556   \n",
       "6     progressive_mlp         False     0.807375    0.797991   \n",
       "7     progressive_mlp          True     0.807375    0.797991   \n",
       "\n",
       "   execution time (s)  \n",
       "0            0.295100  \n",
       "1            0.172753  \n",
       "2            0.174946  \n",
       "3            0.183413  \n",
       "4            0.090874  \n",
       "5            0.110840  \n",
       "6            0.079345  \n",
       "7            0.128248  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# function for handling missing values \n",
    "# and fitting logistic regression on clean data\n",
    "def NB_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    K-Nearest Neighbors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: Accuracy on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # store original columns\n",
    "    original_columns = data.columns.difference(['depressed'])\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state)\n",
    "    \n",
    "    # note which predictor columns were dropped or kept\n",
    "    kept_columns = df_imputed.columns.difference(['depressed'])\n",
    "    dropped_columns = original_columns.difference(df_imputed.columns)\n",
    "    original_columns = original_columns.difference(['depressed'])\n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    nbc = GaussianNB().fit(\n",
    "        X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    train_score = accuracy_score(y_train, nbc.predict(X_train))\n",
    "    test_score = accuracy_score(y_test, nbc.predict(X_test))\n",
    "    duration = timer() - start\n",
    "    y_pred = nbc.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "    #print(\"Classification rate on training data: {}\".format(train_score))\n",
    "    #print(\"Classification rate on test data: {}\".format(test_score))\n",
    "    print(\"Execution time: {}\".format(duration))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'standardized': cols_to_standardize!=None,\n",
    "        'model': nbc,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "        'execution time (s)': duration\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "nbc_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:\n",
    "    for cols in [None, cols_to_standardize]:   \n",
    "        result = NB_model(df, impute_strategy=impute_strategy, cols_to_standardize=cols)\n",
    "        nbc_results.append(result)\n",
    "\n",
    "# display logistic regression performance\n",
    "nbc_results_df = pd.DataFrame(nbc_results)\n",
    "nbc_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5558  244]\n",
      " [ 434   36]]\n",
      "Execution time: 0.2578275019995999\n",
      "[[5800    2]\n",
      " [ 468    2]]\n",
      "Execution time: 0.18169172200032335\n",
      "[[5558  244]\n",
      " [ 434   36]]\n",
      "Execution time: 0.20551894300024287\n",
      "[[5800    2]\n",
      " [ 468    2]]\n",
      "Execution time: 0.17338147800001025\n",
      "[[5795    7]\n",
      " [ 468    2]]\n",
      "Execution time: 0.0916129320003165\n",
      "[[5795    7]\n",
      " [ 468    2]]\n",
      "Execution time: 0.10219282300022314\n",
      "[[5456  346]\n",
      " [ 338  132]]\n",
      "Execution time: 0.0716464970000743\n",
      "[[5456  346]\n",
      " [ 338  132]]\n",
      "Execution time: 0.11942043199996988\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>standardized</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>execution time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>False</td>\n",
       "      <td>0.892486</td>\n",
       "      <td>0.891901</td>\n",
       "      <td>0.257828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>True</td>\n",
       "      <td>0.925095</td>\n",
       "      <td>0.925064</td>\n",
       "      <td>0.181692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>median</td>\n",
       "      <td>False</td>\n",
       "      <td>0.892486</td>\n",
       "      <td>0.891901</td>\n",
       "      <td>0.205519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>median</td>\n",
       "      <td>True</td>\n",
       "      <td>0.925095</td>\n",
       "      <td>0.925064</td>\n",
       "      <td>0.173381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>False</td>\n",
       "      <td>0.925334</td>\n",
       "      <td>0.924267</td>\n",
       "      <td>0.091613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>True</td>\n",
       "      <td>0.925334</td>\n",
       "      <td>0.924267</td>\n",
       "      <td>0.102193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>False</td>\n",
       "      <td>0.895914</td>\n",
       "      <td>0.890944</td>\n",
       "      <td>0.071646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>True</td>\n",
       "      <td>0.895914</td>\n",
       "      <td>0.890944</td>\n",
       "      <td>0.119420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  standardized  train score  test score  \\\n",
       "0                mean         False     0.892486    0.891901   \n",
       "1                mean          True     0.925095    0.925064   \n",
       "2              median         False     0.892486    0.891901   \n",
       "3              median          True     0.925095    0.925064   \n",
       "4     progressive_knn         False     0.925334    0.924267   \n",
       "5     progressive_knn          True     0.925334    0.924267   \n",
       "6     progressive_mlp         False     0.895914    0.890944   \n",
       "7     progressive_mlp          True     0.895914    0.890944   \n",
       "\n",
       "   execution time (s)  \n",
       "0            0.257828  \n",
       "1            0.181692  \n",
       "2            0.205519  \n",
       "3            0.173381  \n",
       "4            0.091613  \n",
       "5            0.102193  \n",
       "6            0.071646  \n",
       "7            0.119420  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# function for handling missing values \n",
    "# and fitting logistic regression on clean data\n",
    "def ppn_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Simple Perceptron Model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: Accuracy on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # store original columns\n",
    "    original_columns = data.columns.difference(['depressed'])\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state)\n",
    "    \n",
    "    # note which predictor columns were dropped or kept\n",
    "    kept_columns = df_imputed.columns.difference(['depressed'])\n",
    "    dropped_columns = original_columns.difference(df_imputed.columns)\n",
    "    original_columns = original_columns.difference(['depressed'])\n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    ppn = Perceptron(max_iter=40, eta0=0.1, random_state=0).fit(X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    train_score = accuracy_score(y_train, ppn.predict(X_train))\n",
    "    test_score = accuracy_score(y_test, ppn.predict(X_test))\n",
    "    duration = timer() - start\n",
    "    y_pred = ppn.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "    #print(\"Classification rate on training data: {}\".format(train_score))\n",
    "    #print(\"Classification rate on test data: {}\".format(test_score))\n",
    "    print(\"Execution time: {}\".format(duration))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'standardized': cols_to_standardize!=None,\n",
    "        'model': ppn,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "        'execution time (s)': duration\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "ppn_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:\n",
    "    for cols in [None, cols_to_standardize]:   \n",
    "        result = ppn_model(df, impute_strategy=impute_strategy, cols_to_standardize=cols)\n",
    "        ppn_results.append(result)\n",
    "\n",
    "# display logistic regression performance\n",
    "ppn_results_df = pd.DataFrame(ppn_results)\n",
    "ppn_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25085 samples, validate on 6272 samples\n",
      "Epoch 1/1\n",
      "25085/25085 [==============================] - 7s 276us/step - loss: 2.3310 - accuracy: 0.8785 - val_loss: 0.2816 - val_accuracy: 0.9251\n",
      "Test loss: 0.28155736009381255\n",
      "Test accuracy: 0.9250637888908386\n",
      "Train on 25085 samples, validate on 6272 samples\n",
      "Epoch 1/1\n",
      "25085/25085 [==============================] - 7s 279us/step - loss: 0.2637 - accuracy: 0.9185 - val_loss: 0.2310 - val_accuracy: 0.9249\n",
      "Test loss: 0.23096365437899924\n",
      "Test accuracy: 0.9249043464660645\n",
      "Train on 25085 samples, validate on 6272 samples\n",
      "Epoch 1/1\n",
      "25085/25085 [==============================] - 7s 277us/step - loss: 2.8455 - accuracy: 0.8716 - val_loss: 0.2997 - val_accuracy: 0.9251\n",
      "Test loss: 0.29969514127136493\n",
      "Test accuracy: 0.9250637888908386\n",
      "Train on 25085 samples, validate on 6272 samples\n",
      "Epoch 1/1\n",
      "25085/25085 [==============================] - 7s 282us/step - loss: 0.2625 - accuracy: 0.9213 - val_loss: 0.2471 - val_accuracy: 0.9180\n",
      "Test loss: 0.2470599681291045\n",
      "Test accuracy: 0.9180484414100647\n",
      "Train on 25085 samples, validate on 6272 samples\n",
      "Epoch 1/1\n",
      "25085/25085 [==============================] - 7s 280us/step - loss: 0.2672 - accuracy: 0.9201 - val_loss: 0.2385 - val_accuracy: 0.9246\n",
      "Test loss: 0.23849957213946144\n",
      "Test accuracy: 0.9245854616165161\n",
      "Train on 25085 samples, validate on 6272 samples\n",
      "Epoch 1/1\n",
      "25085/25085 [==============================] - 7s 295us/step - loss: 0.2658 - accuracy: 0.9195 - val_loss: 0.2332 - val_accuracy: 0.9244\n",
      "Test loss: 0.2332398233441066\n",
      "Test accuracy: 0.9244260191917419\n",
      "Train on 25085 samples, validate on 6272 samples\n",
      "Epoch 1/1\n",
      "25085/25085 [==============================] - 7s 284us/step - loss: 0.2632 - accuracy: 0.9204 - val_loss: 0.2362 - val_accuracy: 0.9243\n",
      "Test loss: 0.236212224509491\n",
      "Test accuracy: 0.9242665767669678\n",
      "Train on 25085 samples, validate on 6272 samples\n",
      "Epoch 1/1\n",
      "25085/25085 [==============================] - 9s 364us/step - loss: 0.2682 - accuracy: 0.9182 - val_loss: 0.2597 - val_accuracy: 0.9209\n",
      "Test loss: 0.25965843486542606\n",
      "Test accuracy: 0.920918345451355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imputation strategy</th>\n",
       "      <th>standardized</th>\n",
       "      <th>Test loss</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>False</td>\n",
       "      <td>0.281557</td>\n",
       "      <td>0.925064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>True</td>\n",
       "      <td>0.230964</td>\n",
       "      <td>0.924904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>median</td>\n",
       "      <td>False</td>\n",
       "      <td>0.299695</td>\n",
       "      <td>0.925064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>median</td>\n",
       "      <td>True</td>\n",
       "      <td>0.247060</td>\n",
       "      <td>0.918048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>False</td>\n",
       "      <td>0.238500</td>\n",
       "      <td>0.924585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>progressive_knn</td>\n",
       "      <td>True</td>\n",
       "      <td>0.233240</td>\n",
       "      <td>0.924426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>False</td>\n",
       "      <td>0.236212</td>\n",
       "      <td>0.924267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>progressive_mlp</td>\n",
       "      <td>True</td>\n",
       "      <td>0.259658</td>\n",
       "      <td>0.920918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  imputation strategy  standardized  Test loss  Test accuracy\n",
       "0                mean         False   0.281557       0.925064\n",
       "1                mean          True   0.230964       0.924904\n",
       "2              median         False   0.299695       0.925064\n",
       "3              median          True   0.247060       0.918048\n",
       "4     progressive_knn         False   0.238500       0.924585\n",
       "5     progressive_knn          True   0.233240       0.924426\n",
       "6     progressive_mlp         False   0.236212       0.924267\n",
       "7     progressive_mlp          True   0.259658       0.920918"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# function for handling missing values \n",
    "# and fitting logistic regression on clean data\n",
    "def keras_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    Keras Perceptron Model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: Accuracy on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    batch_size = 128\n",
    "    num_classes = 10\n",
    "    epochs = 1\n",
    "    \n",
    "    # store original columns\n",
    "    original_columns = data.columns.difference(['depressed'])\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state)\n",
    "    \n",
    "    # note which predictor columns were dropped or kept\n",
    "    kept_columns = df_imputed.columns.difference(['depressed'])\n",
    "    dropped_columns = original_columns.difference(df_imputed.columns)\n",
    "    original_columns = original_columns.difference(['depressed'])\n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(len(kept_columns),)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "    \n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    # model evaulation\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'standardized': cols_to_standardize!=None,\n",
    "        'model': keras_model,\n",
    "        'Test loss' :  score[0],\n",
    "        'Test accuracy' : score[1]\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "keras_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn', 'progressive_mlp']:\n",
    "    for cols in [None, cols_to_standardize]:   \n",
    "        result = keras_model(df, impute_strategy=impute_strategy, cols_to_standardize=cols)\n",
    "        keras_results.append(result)\n",
    "\n",
    "# display logistic regression performance\n",
    "keras_results_df = pd.DataFrame(keras_results)\n",
    "keras_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# function for handling missing values \n",
    "# and fitting logistic regression on clean data\n",
    "def svm_model(data, impute_strategy=None,\n",
    "                        cols_to_standardize=None,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42):\n",
    "    \"\"\"\n",
    "    SVM\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: dataframe\n",
    "    impute_strategy: call impute_data() function for mean, median, or progressive_knn imputation\n",
    "    cols_to_standardize: continous variables\n",
    "    test_size: train-test split proportion\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    prints confusion matrix\n",
    "    train_score, test_score: Accuracy on training and testing set\n",
    "    reports time elapsed\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # store original columns\n",
    "    original_columns = data.columns.difference(['depressed'])\n",
    "    df_imputed = impute_data(data, impute_strategy, cols_to_standardize)\n",
    "    train_data, test_data = train_test_split(df_imputed, test_size=test_size,\n",
    "                                             random_state=random_state)\n",
    "    \n",
    "    # note which predictor columns were dropped or kept\n",
    "    kept_columns = df_imputed.columns.difference(['depressed'])\n",
    "    dropped_columns = original_columns.difference(df_imputed.columns)\n",
    "    original_columns = original_columns.difference(['depressed'])\n",
    "    \n",
    "    # prepare tensors\n",
    "    X_train = train_data.drop(columns=['depressed'])\n",
    "    y_train = train_data['depressed']\n",
    "    X_test = test_data.drop(columns=['depressed'])\n",
    "    y_test = test_data['depressed']\n",
    "    \n",
    "    # model training\n",
    "    svm = SVC(kernel='linear', class_weight='balanced', # penalize\n",
    "            probability=True, random_state=1, gamma=0.2, C=1.0).fit(X_train, y_train)\n",
    "    \n",
    "    # model evaluation\n",
    "    train_score = accuracy_score(y_train, svm.predict(X_train))\n",
    "    test_score = accuracy_score(y_test, svm.predict(X_test))\n",
    "    duration = timer() - start\n",
    "    y_pred = svm.predict(X_test)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "    #print(\"Classification rate on training data: {}\".format(train_score))\n",
    "    #print(\"Classification rate on test data: {}\".format(test_score))\n",
    "    print(\"Execution time: {}\".format(duration))\n",
    "    \n",
    "    return {\n",
    "        'imputation strategy': impute_strategy,\n",
    "        'standardized': cols_to_standardize!=None,\n",
    "        'model': ppn,\n",
    "        'train score': train_score,\n",
    "        'test score': test_score,\n",
    "        'execution time (s)': duration\n",
    "    }\n",
    "  \n",
    "# list to store models' performance  \n",
    "svm_results = []\n",
    "\n",
    "# prepare data\n",
    "df = df_raw\n",
    "cols_to_standardize = cont\n",
    "\n",
    "# fit logistic regression for each imputation strategy\n",
    "# with and without standardizing features\n",
    "for impute_strategy in ['mean', 'median', 'progressive_knn']:\n",
    "    for cols in [None, cols_to_standardize]:   \n",
    "        result = svm_model(df, impute_strategy=impute_strategy, cols_to_standardize=cols)\n",
    "        svm_results.append(result)\n",
    "\n",
    "# display logistic regression performance\n",
    "svm_results_df = pd.DataFrame(svm_results)\n",
    "svm_results_df.drop(['model'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best RF model for feature importance\n",
    "# {'model__min_samples_leaf': 1, 'model__min_samples_split': 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(testy))]\n",
    "# fit a model\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "model.fit(trainX, trainy)\n",
    "# predict probabilities\n",
    "lr_probs = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(testy, ns_probs)\n",
    "lr_auc = roc_auc_score(testy, lr_probs)\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "78nn7ejt4AOI",
    "IE9wOLk84AOM",
    "ei8r0bJP4AOU",
    "_qlzJj8P4AOW",
    "qzgPDkxM4AOg",
    "aFZrDH4G4AOn",
    "n1wNsrxv4AOr",
    "FAnUmBDm4AOv"
   ],
   "name": "case_study_4_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
